<!DOCTYPE HTML>
<html lang="ja">
<head>
	<meta charset="UTF-8">
	<link rel="stylesheet" href="docs.css" type="text/css" />
    <script src="prettify.js" type="text/javascript"></script>
    <link href="prettify.css" rel="stylesheet" type="text/css"/>
	<title>HGNN</title>
</head>
<body onload="prettyPrint()">
<div id=title>HGNN - HalfGammon Neural Network -</div>
<div id=author>Copyright (C) 2020 by N.Tsuda</div>

<h2>概要</h2>
<p>「HGNN - HalfGammon Neural Network -」は、人工ニューラルネット（ANN）を使用し、ハーフギャモンの状態から得点期待値を予測する</p>
<ul>
	<li>「人工ニューラルネット（Artificial Neural Network）」は、生物の脳細胞を簡略化したニューロンをネットワーク化したもの</li>
	<li>「<a href="http://vivi.dyndns.org/games/HalfGammon/">ハーフギャモン</a>」とは、
	通常の<a href="https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%83%E3%82%AF%E3%82%AE%E3%83%A3%E3%83%A2%E3%83%B3">バックギャモン</a>
	の盤面サイズ・石数・ダイスの目を（ほぼ）半分にしたボードゲーム</li>
	<li>ハーフギャモンは、通常の勝ち負け以外に、終局時のある条件によりギャモン勝ち、バックギャモン勝ちがあり、それぞれの得点は±１，±２，±３となる</li>
</ul>

<h3>リンク</h3>
<ul>
	<li>プロジェクトのダウンロードは <a href="https://github.com/vivisuke/HGNN">github</a> から</li>
</ul>

<h3>目次</h3>
<ul>
	<li><a href="#HowToUse">HGNN 使用方法</a></li>
	<ul>
		<li><a href="#constructNN">ニューラルネット構築</a></li>
		<li><a href="#learning">学習</a></li>
		<li><a href="#predict">予測</a></li>
	</ul>
	<li><a href="#example">HGNN 使用例</a></li>
	<ul>
		<li><a href="#linearFunc">線形関数</a></li>
		<li><a href="#sin">sin関数</a></li>
	</ul>
</ul>

<h2 id="HowToUse">HGNN 使用方法</h2>
<p>ビルド環境：Visual Studio 2019、C++</p>
<p>HGNN の使用方法は以下の通り</p>
<ol>
	<li>人工ニューラルネット（ANN）構築</li>
	<li>入力・教師値データを使って学習</li>
	<li>入力を与え、結果を予測</li>
</ol>
<!--<p>以下、具体的なソースコードを示す</p>-->

<h3 id="constructNN">ニューラルネット構築</h3>
<p>ニューラルネットを構築するコード例を以下に示す。</p>
<pre class="prettyprint">
#include &lt;vector&gt;
#include "HGNNet.h"
using namespace std;
.....
    HGNNet nn;     // ニューラルネットオブジェクト生成
    nn.init(vector&lt;int&gt;{2, 10}, TANH);   // 入力層：２ノード、１隠れ層・10ノード、活性化関数：tanh
</pre>
<p>最初に標準ライブラリの vector をインクルードする。これはニューラルネットの層数・各層のノード数を指定するために使用する。</p>
<p>ニューラルネットクラス HGNNet のヘッダもインクルードしておく。</p>
<p>「using namespace std;」は好みで記述するといいだろう。記述しておけば、以下「std::」を省略できる。</p>
<p>「HGNNet nn;」で、ニューラルネットオブジェクトを生成する。引数は特に必要ない。</p>
<p>ついで、HGNNet::init(vector&lt;int&gt;&amp;, ActFunc) をコールし、ニューラルネットを初期化する。重み係数は [-1, +1] でランダムに初期化される。
<br>第一引数は層数・各層のノード数を指定する。HGNNet は出力に回帰だけをサポートしており、出力ノードは１固定なので、出力ノードは指定しない。
入力層ノード数に続けて、隠れ層の分だけ各層のノード数を記述する。下記のように vector オブジェクトの生成を分けてもいいが、１行で書く方が簡潔で筆者の好みだ。</p>
<pre class="prettyprint">
    vector&lt;int&gt; nodes = {2, 10};
    nn.init(nodes, TANH);
</pre>
<p>隠れ層が複数ある場合は、その数だけ隠れ層のノード数を記述する。下記は隠れ層が２層の場合の例だ。</p>
<pre class="prettyprint">
    nn.init(vector&lt;int&gt;{2, 10, 10}, TANH);
</pre>
<p>HGNNet::init() の第２引数には活性化関数を指定する。SIGMOID, TANH, RELU が指定可能だ。各々、シグモイド関数、tanh関数、ReLU関数を示す。
<br>これらの活性化関数は隠れ層の演算の時に用いられる。HGNNet の出力は回帰なので、活性化関数は適用されない（あえて言えば恒等関数だ）。</p>

<h3 id="learning">学習</h3>
<pre class="prettyprint">
    vector&lt;data_t&gt; input;
    input に入力データを設定
    data_t T = 教師値;
    nn.train(input, T);
</pre>
<p>学習（トレーニング）を行うには、上記の様に、入力データを vector&lt;data_t&gt; に設定し、それと教師値を引数にして HGNNet::train() をコールする。
<br>data_t は HGNNet.h で宣言されたデータ型識別子で、double が割り当てられている。メモリが不足する場合などは float に定義し直すという選択肢もある。
<br>学習を行うと誤差逆伝播法を用いてネットワークの重み係数が教師値に近づくように少し変化する。</p>
<p>train() の第３引数で係数修正の度合いを学習率で指定できる。デフォルトは 0.01 である。この値が大きいと学習が速く進むが、その反面発散確率が高まる。
<br>なので、徐々に学習率を下げながら学習を行うというテクニック（adagrad 等）も存在する。</p>
<pre class="prettyprint">
    vector&lt;data_t&gt; input;
    input に入力データを設定
    data_t T = 教師値;
    nn.train(input, T, 0.001);       //  学習係数指定
</pre>

<h3 id="predict">予測</h3>
<p>学習が終わったネットワークに対し、nn.predict() をコールすることで、入力値から教師値に近い値を予測することができる。</p>
<p>コードは以下のように、vector&lt;data_t&gt; に入力データを設定し、predict() をコールすると、予測値を返してくれる。</p>
<pre class="prettyprint">
    vector&lt;data_t&gt; input;
    input に入力データを設定
    auto v = nn.predict(input);
</pre>

<h2 id="example">HGNN 使用例</h2>

<h3 id="linearFunc">線形関数</h3>
<p>「y = 3*x1 - 2*x2 + 1」、２層NN</p>
<pre class="prettyprint">
data_t linearFunc(data_t x1, data_t x2) { return x1*3 - x2*2 + 1; }
double linearRMS(HGNNet&amp; nn, int N_LOOP = 100)
{
    vector&lt;double&gt; input(2);
    double sum2 = 0;
    for (int i = 0; i &lt; N_LOOP; ++i) {
        input[0] = g_rand11(g_mt);       // [-1, +1]
        input[1] = g_rand11(g_mt);       // [-1, +1]
        double err = nn.predict(input) - linearFunc(input[0], input[1]);
        sum2 += err * err;
    }
    return sqrt(sum2 / N_LOOP);
}
void test_linearFunc()
{
    HGNNet nn;
    cout &lt;&lt; "# node of layers: {2 1}\n\n";
    nn.init(vector&lt;int&gt;{2}, SIGMOID);    // ２入力のみ（隠れ層無し）
    vector&lt;double&gt; input(2);
    // 学習・評価
    cout &lt;&lt; "N\tRMS\n";
    cout &lt;&lt; "------- ----------\n";
    for (int cnt = 1; cnt &lt;= 10000; ++cnt) {
        input[0] = g_rand11(g_mt);       // [-1, +1]
        input[1] = g_rand11(g_mt);       // [-1, +1]
        nn.train(input, linearFunc(input[0], input[1]));
        if( log10(cnt) == (int)log10(cnt) )
        {
            cout &lt;&lt; "10^" &lt;&lt; log10(cnt) &lt;&lt; "\t" &lt;&lt; linearRMS(nn) &lt;&lt; endl;
        }
    }
    cout &lt;&lt; "\n" &lt;&lt; nn.dump() &lt;&lt; "\n";
}
</pre>


<h3 id="sinFunc">sin関数</h3>
<p></p>


</body>
</html>
